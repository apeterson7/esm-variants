{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install -q pandas numpy tqdm torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aUV6kgdrA5m6"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "class ProteinLanguageModelPredictor():\n",
        "\n",
        "  def __init__(self, model_name, repr_layer=33, device=0, silent=False):\n",
        "    self.AAorder=['K','R','H','E','D','N','Q','T','S','C','G','A','V','L','I','M','P','Y','F','W']\n",
        "    self.device = device\n",
        "    self.silent = silent\n",
        "    self.repr_layer = repr_layer\n",
        "    model, alphabet = torch.hub.load(\"facebookresearch/esm:main\", model_name)\n",
        "    self.batch_converter = alphabet.get_batch_converter() \n",
        "    self.model = model.eval().to(device)\n",
        "    self.alphabet = alphabet\n",
        "\n",
        "  def get_wt_LLR(self, input_df: pd.DataFrame) -> Tuple[list, list]: \n",
        "    # input: df.columns= id,\tgene,\tseq, length\n",
        "    # requires global model and batch_converter\n",
        "    # make sure input_df does not contain any nonstandard amino acids\n",
        "    genes = input_df.id.values\n",
        "    llr_dfs=[]\n",
        "    for gene_name in tqdm(genes,disable=self.silent):\n",
        "      input_sequence = input_df[input_df.id==gene_name].seq.values[0]\n",
        "      input_sequence_length = len(input_sequence)\n",
        "      \n",
        "      if input_sequence_length<=1022:\n",
        "        logits = self._get_logits(input_sequence, gene_name)\n",
        "      else: ### tiling\n",
        "        logits = self._get_logits_with_tiling(input_sequence, gene_name)\n",
        "\n",
        "      llr_df = self._create_llr_dataframe(input_sequence, logits)\n",
        "      llr_dfs.append(llr_df)\n",
        "\n",
        "    return genes, llr_dfs\n",
        "\n",
        "  def _get_logits(self, seq: str, gene_name: str = \"\"):\n",
        "    data = [ (gene_name+\"_\", seq),]\n",
        "    _, _, batch_tokens = self.batch_converter(data)\n",
        "    batch_tokens = batch_tokens.to(self.device)\n",
        "    with torch.no_grad():\n",
        "      logits = torch.log_softmax(self.model(batch_tokens, repr_layers=[33], return_contacts=False)[\"logits\"],dim=-1).cpu().numpy()\n",
        "      return logits[0][1:-1,:]\n",
        "\n",
        "  def _get_logits_with_tiling(self, input_sequence: str, gene_name: str):\n",
        "    intervals, M_norm = self.get_intervals_and_weights(len(input_sequence),min_overlap=512,max_len=1022,s=20)\n",
        "        \n",
        "    dt = []\n",
        "    for i,idx in enumerate(intervals):\n",
        "      dt += [(gene_name+'_WT_'+str(i),''.join(list(np.array(list(input_sequence))[idx])) )]\n",
        "\n",
        "    logit_parts = []\n",
        "    for dt_ in self.chunks(dt,20):\n",
        "      _, _, batch_tokens = self.batch_converter(dt_)\n",
        "      with torch.no_grad():\n",
        "        results_ = torch.log_softmax(self.model(batch_tokens.to(self.device), repr_layers=[33], return_contacts=False)['logits'],dim=-1)\n",
        "      for i in range(results_.shape[0]):\n",
        "        logit_parts += [results_[i,:,:].cpu().numpy()[1:-1,:]]\n",
        "        \n",
        "    logits_full = np.zeros((len(input_sequence),33))\n",
        "    for i in range(len(intervals)):\n",
        "      logit = np.zeros((len(input_sequence),33))\n",
        "      logit[intervals[i]] = logit_parts[i].copy()\n",
        "      logit = np.multiply(logit.T, M_norm[i,:]).T\n",
        "      logits_full+=logit\n",
        "    \n",
        "    return logits_full\n",
        "\n",
        "  def _create_llr_dataframe(self, input_sequence: str, logits) -> pd.DataFrame:\n",
        "    wt_logits_df = pd.DataFrame(\n",
        "      logits,\n",
        "      columns=self.alphabet.all_toks,\n",
        "      index=list(input_sequence)\n",
        "    ).T.iloc[4:24].loc[self.AAorder]\n",
        "\n",
        "    wt_logits_df.columns = [j.split('.')[0]+' '+str(i+1) for i,j in enumerate(wt_logits_df.columns)]\n",
        "    wt_norm=np.diag(wt_logits_df.loc[[i.split(' ')[0] for i in wt_logits_df.columns]])\n",
        "    llr_df = wt_logits_df - wt_norm\n",
        "    return llr_df\n",
        "\n",
        "  ##################### TILING utils ###########################\n",
        "\n",
        "  def chunks(self, lst: list, n: int):\n",
        "      \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
        "      for i in range(0, len(lst), n):\n",
        "          yield lst[i:i + n]\n",
        "\n",
        "  def chop(self, L,min_overlap=511,max_len=1022):\n",
        "    return L[max_len-min_overlap:-max_len+min_overlap]\n",
        "    \n",
        "  def intervals(self, L,min_overlap=511,max_len=1022,parts=[]):\n",
        "    #print('L:',len(L))\n",
        "    #print(len(parts))\n",
        "    if len(L)<=max_len:\n",
        "      if parts[-2][-1]-parts[-1][0]<min_overlap:\n",
        "        #print('DIFF:',parts[-2][-1]-parts[-1][0])\n",
        "        return parts+[np.arange(L[int(len(L)/2)]-int(max_len/2),L[int(len(L)/2)]+int(max_len/2)) ]\n",
        "      else:\n",
        "        return parts\n",
        "    else:\n",
        "      parts+=[L[:max_len],L[-max_len:]]\n",
        "      L=self.chop(L,min_overlap,max_len)\n",
        "      return self.intervals(L,min_overlap,max_len,parts=parts)\n",
        "\n",
        "  def get_intervals_and_weights(self, seq_len, min_overlap=512, max_len=1022, s=20):\n",
        "    intervals = self.intervals(np.arange(seq_len), min_overlap=min_overlap, max_len=max_len)\n",
        "    ## sort intervals\n",
        "    intervals = [intervals[i] for i in np.argsort([i[0] for i in intervals])]\n",
        "\n",
        "    a=int(np.round(min_overlap/2))\n",
        "    t=np.arange(max_len)\n",
        "\n",
        "    f=np.ones(max_len)\n",
        "    f[:a] = 1 / (1 + np.exp(-(t[:a]-a/2)/s))\n",
        "    f[max_len-a:] = 1 / (1 + np.exp((t[:a]-a/2)/s))\n",
        "\n",
        "    f0=np.ones(max_len)\n",
        "    f0[max_len-a:] = 1 / (1 + np.exp((t[:a]-a/2)/s))\n",
        "\n",
        "    fn=np.ones(max_len)\n",
        "    fn[:a] = 1 / (1 + np.exp(-(t[:a]-a/2)/s))\n",
        "\n",
        "    filt=[f0]+[f for i in intervals[1:-1]]+[fn]\n",
        "    M = np.zeros((len(intervals),seq_len))\n",
        "    for k,i in enumerate(intervals):\n",
        "      M[k,i] = filt[k]\n",
        "    M_norm = M/M.sum(0)\n",
        "    return (intervals, M_norm)\n",
        "\n",
        "  \n",
        "  ##################### Psuedo Log Likelihood ###########################\n",
        "\n",
        "  def get_PLL(self, seq: str, reduce = np.sum):\n",
        "    s=self._get_logits(seq)\n",
        "    idx=[self.alphabet.tok_to_idx[i] for i in seq]\n",
        "    return reduce(np.diag(s[:,idx])) \n",
        "\n",
        "  ## PLLR score for indels\n",
        "  def get_PLLR(self,wt_seq,mut_seq,start_pos,weighted=False):\n",
        "    fn=np.sum if not weighted else np.mean\n",
        "    if max(len(wt_seq),len(mut_seq))<=1022:\n",
        "      return self.get_PLL(mut_seq,fn) - self.get_PLL(wt_seq,fn)\n",
        "    else:\n",
        "      wt_seq,mut_seq,start_pos = self.crop_indel(wt_seq,mut_seq,start_pos)\n",
        "      return self.get_PLL(mut_seq,fn) - self.get_PLL(wt_seq,fn)\n",
        "\n",
        "  def crop_indel(self, ref_seq,alt_seq,ref_start):\n",
        "    # Start pos: 1-indexed start position of variant\n",
        "    left_pos = ref_start-1\n",
        "    offset = len(ref_seq)-len(alt_seq)\n",
        "    start_pos = int(left_pos - 1022 / 2)\n",
        "    end_pos1 = int(left_pos + 1022 / 2) -min(start_pos,0)+ min(offset,0)\n",
        "    end_pos2 = int(left_pos + 1022 / 2) -min(start_pos,0)- max(offset,0)\n",
        "    if start_pos < 0: start_pos = 0 # Make sure the start position is not negative\n",
        "    if end_pos1 > len(ref_seq): end_pos1 = len(ref_seq) # Make sure the end positions are not beyond the end of the sequence\n",
        "    if end_pos2 > len(alt_seq): end_pos2 = len(alt_seq)\n",
        "    if start_pos>0 and max(end_pos2,end_pos1) - start_pos <1022: ## extend to the left if there's space\n",
        "              start_pos = max(0,max(end_pos2,end_pos1)-1022)\n",
        "\n",
        "    return ref_seq[start_pos:end_pos1],alt_seq[start_pos:end_pos2],start_pos-ref_start\n",
        "\n",
        "  ## stop gain variant score\n",
        "  def get_minLLR(self, seq,stop_pos):\n",
        "    return min(self.get_wt_LLR(pd.DataFrame([('_','_',seq,len(seq))],columns=['id','gene','seq','length'] ))[1][0].values[:,stop_pos:].reshape(-1))\n",
        "  \n",
        "  ##################### MELTed CSV ###########################\n",
        "\n",
        "  def meltLLR(self, LLR,savedir=None):\n",
        "    vars = LLR.melt(ignore_index=False)\n",
        "    vars['variant'] = [''.join(i.split(' '))+j for i,j in zip(vars['variable'],vars.index)]\n",
        "    vars['score'] = vars['value']\n",
        "    vars = vars.set_index('variant')\n",
        "    vars['pos'] = [int(i[1:-1]) for i in vars.index]\n",
        "    del vars['variable'],vars['value']\n",
        "    if savedir is not None:\n",
        "        vars.to_csv(savedir+'var_scores.csv')\n",
        "    return vars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZShMiadyO-uq"
      },
      "source": [
        "### example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clIbHRwrCYww",
        "outputId": "ba5bb3bd-5ea9-4688-db0d-218b4d6190df"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /Users/alexanderpeterson/.cache/torch/hub/facebookresearch_esm_main\n"
          ]
        }
      ],
      "source": [
        "#Load model\n",
        "plmPredictor = ProteinLanguageModelPredictor(model_name='esm1b_t33_650M_UR50S')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "45N6RSGoPB-m"
      },
      "outputs": [],
      "source": [
        "##create a toy dataset\n",
        "df_in = pd.DataFrame([('P1','gene1','FISHWISHFQRCHIPSTHATARECRISP',28),\n",
        "                      ('P2','gene2','RAGEAGAINSTTHEMACHINE',21),\n",
        "                      ('P3','gene3','SHIPSSAILASFISHSWIM',19),\n",
        "                      ('P4','gene4','A'*1948,1948)], columns = ['id','gene','seq','length'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noYpT2fbPEFc",
        "outputId": "028259ed-0858-4d13-93cd-fc50b75f4170"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:21<00:00,  5.49s/it]\n"
          ]
        }
      ],
      "source": [
        "## Get LLRs\n",
        "ids,LLRs = plmPredictor.get_wt_LLR(df_in)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ubRTY_4Pr2L",
        "outputId": "29722d7a-7cb9-409d-808c-99e9577a5210"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "P1 (20, 28)\n",
            "P2 (20, 21)\n",
            "P3 (20, 19)\n",
            "P4 (20, 1948)\n"
          ]
        }
      ],
      "source": [
        "for i,LLR in zip(ids,LLRs):\n",
        "  print(i,LLR.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wk8M2c31PtmA",
        "outputId": "7d617123-4831-41df-a749-462fe200aa09"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.float32(-16.3117)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "plmPredictor.get_PLL(df_in.seq.values[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vP2KRAC6AaRS",
        "outputId": "a7064e66-d9de-49b4-9cfe-13a759737561"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.float32(-1.0077915)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# indel 14_IPS_delins_EESE (FISHWISHFQRCHIPSTHATARECRISP --> FISHWISHFQRCHEESETHATARECRISP)\n",
        "plmPredictor.get_PLLR('FISHWISHFQRCHIPSTHATARECRISP','FISHWISHFQRCHEESETHATARECRISP',14)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydAO4K9HAOX2",
        "outputId": "f94a6537-c0b7-463e-c083-5051ad367348"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  8.96it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "np.float32(-5.1914926)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# stop at position 17\n",
        "plmPredictor.get_minLLR(df_in.seq.values[0],17)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_WwmsxmIDZ5p"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
